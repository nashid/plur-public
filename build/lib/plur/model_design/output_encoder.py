# Copyright 2022 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Output text encoder for PLUR.
"""
from typing import Sequence

from plur.utils import constants


class Encoder():
  """Encoder for vocabulary generated by PLUR.

  The encoder tries to mimic the TokenTextEncoder in tensor2tensor, but with a
  few differences. The main difference is how the encode() function is
  implemented. This is how the encode() function is implemented in
  TokenTextEncoder:

  def encode(self, s):
    sentence = s
    tokens = sentence.strip().split()
    if self._replace_oov is not None:
      tokens = [t if t in self._token_to_id else self._replace_oov
                for t in tokens]
    ret = [self._token_to_id[tok] for tok in tokens]
    return ret[::-1] if self._reverse else ret

  For source code, sentence.strip().split() can cause some unexpected behavior.
  1) strip() removes leading and trailing whitespaces. For some source code
  tokenizer, the whitespace is actually a part of the token. Removing them
  can cause us to have a duplicate vocabulary.
  2) split() splits the sentence into a list of tokens. It assumes that the
  sentence is whitespace separated tokens. However, in source code tokens, it
  is very common that a token has whitespaces in it. For example for string
  literal it is very common that they have whitespaces in them. There is no way
  to tell the encode function of TokenTextEncoder that the input sentence is
  a single token, even if it has whitespaces in it.

  So in this version of encoder, the encode function that we implement treat
  the input string as it is without any modification. The return type of encode
  and decode function are the same as encode and decode function in
  TokenTextEncoder.
  """

  def __init__(self, filename):
    """Build vocab mappings for vocabulary stored in filename."""
    self.vocab_dict = {}
    with open(filename, 'rt') as f:
      lines = f.read().splitlines()
    for idx, line in enumerate(lines):
      self.vocab_dict[line] = idx
    self.reverse_vocab_dict = {v: k for k, v in self.vocab_dict.items()}

  def encode(self, string: str) -> Sequence[int]:
    """Encode the input string to an integer.

    Args:
      string: A string to be encoded.

    Returns:
      A list with a single integer, the integer is the vocab id of the input
      string. We return a list to keep it consistent with tensor2tensor
      TokenTextEncoder.
    """
    oov_id = self.vocab_dict[constants.OOV_TOKEN]
    vocab_id = [self.vocab_dict.get(string, oov_id)]
    return vocab_id

  def decode(self, list_of_token_ids: Sequence[int]) -> str:
    """Decode a list of vocab ids.

    Args:
      list_of_token_ids: A list of vocab ids.

    Returns:
      A string of whitespace-separated tokens, where each token is decoded using
      the vocabulary of the encoder.
    """
    vocab_str = []
    for token_id in list_of_token_ids:
      vocab_token = self.reverse_vocab_dict.get(token_id)
      assert vocab_token is not None, (
          f'The encoder was asked to decode token_id: `{token_id}`, and it '
          f'decoded it as {None}, which was unexpected, since we should not '
          f'be decoding unknown token IDs. Vocabulary is {self.vocab_dict} '
          f'and reverse vocabulary is {self.reverse_vocab_dict}.')
      vocab_str.append(vocab_token)
    return ' '.join(vocab_str)
