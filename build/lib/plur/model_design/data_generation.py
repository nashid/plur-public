# Copyright 2022 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Code pertaining to data generation."""


import functools
import os
import numpy as np

from plur.model_design import data_types as dt
from plur.model_design import output_encoder
from plur.plur_data_loader import PlurDataLoader
from plur.utils import constants


class TocopoDataGenerator(PlurDataLoader):
  """The Plur data loader compatible with the provided models."""

  def __call__(self):
    """Get data from TFRecordDataset, and convert it to data structure in dt."""
    tensor_dict = next(self.dataset_iter)

    batched_train_tocopo_target_data = dt.BatchedTrainTocopoTargetData(
        token_ids=dt.NDArrayIntBO(
            tensor_dict[constants.TARGET_TOKEN_IDS_TENSOR_NAME]),
        is_target_copy=dt.NDArrayBoolBOV(
            tensor_dict[constants.COPY_INDICES_TENSOR_NAME]),
        is_target_pointer=dt.NDArrayBoolBOV(
            tensor_dict[constants.POINTER_INDICES_TENSOR_NAME]))

    # Add a time axis at the second dimension to all input data.
    token_ids = dt.NDArrayIntBTVS(
        tensor_dict[constants.NODE_ID_SEQUENCES_TENSOR_NAME][:, np.newaxis])
    type_ids = dt.NDArrayIntBTV(
        tensor_dict[constants.NODE_TYPES_TENSOR_NAME][:, np.newaxis])
    pointer_candidates = dt.NDArrayBoolBTV(
        tensor_dict[constants.MASKING_CANDIDATE_TENSOR_NAME][:, np.newaxis])
    batched_train_graph_node_data = dt.BatchedTrainGraphNodeData(
        token_ids=token_ids,
        type_ids=type_ids,
        pointer_candidates=pointer_candidates)
    # Convert edges from BEVV to BTEVV.
    edges = dt.NDArrayBoolBTEVV(
        tensor_dict[constants.EDGE_INDICATORS_TENSOR_NAME][:, np.newaxis])

    time_edges = dt.NDArrayBoolBVETT(
        np.zeros((edges.shape[0], edges.shape[3], edges.shape[2], 1, 1),
                 dtype=np.bool))
    batched_train_graph_edge_data = dt.BatchedTrainGraphEdgeData(
        edges=edges, time_edges=time_edges)

    batched_train_tocopo_data = dt.BatchedTrainTocopoData(
        node_data=batched_train_graph_node_data,
        edge_data=batched_train_graph_edge_data,
        target_data=batched_train_tocopo_target_data)

    # Add time axis to node texts in eval data.
    node_texts = dt.NDArrayObjBTV(
        tensor_dict[constants.NODE_TEXT_SEQUENCES_TENSOR_NAME][:, np.newaxis])
    batched_eval_graph_node_data = dt.BatchedEvalGraphNodeData(
        node_texts=node_texts)
    batched_eval_tocopo_target_data = dt.BatchedEvalTocopoTargetData(
        tokens=dt.NDArrayObjBO(tensor_dict[constants.TARGET_TEXTS_TENSOR_NAME]))
    batched_eval_tocopo_data = dt.BatchedEvalTocopoData(
        node_data=batched_eval_graph_node_data,
        target_data=batched_eval_tocopo_target_data,
        provenance=dt.NDArrayObjB(
            tensor_dict[constants.PROVENANCE_TENSOR_NAME]))

    return batched_train_tocopo_data, batched_eval_tocopo_data


def get_plur_data_generator_and_padding_spec(stage_2_dir: str,
                                             batch_size: int,
                                             drop_remainder: bool = False):
  """Data generator for dataset generated by PLUR.

  Args:
    stage_2_dir: Path to PLUR stage 2 directory.
    batch_size: The size of the batches.
    drop_remainder:  If set to True, drop the last batch if the batch is smaller
      than batch_size.

  Returns:
  A tuple of (TrainDataGenerator_fn, ValidDataGenerator_fn,
    TestDataGenerator_fn, Graph2TocopoPaddingSpec). Where the DataGenerator_fn
    is a function that creates the corresponding data generator.
  """
  dummy_dataset = TocopoDataGenerator(
      stage_2_dir, constants.TRAIN_SPLIT_NAME, 0, 0, True, create_dataset=False)

  padding_spec = dt.Graph2TocopoPaddingSpec(
      num_nodes_per_graph=dummy_dataset.max_num_nodes,
      num_edge_types=dummy_dataset.max_num_edge_types,
      output_length=dummy_dataset.max_num_output)

  train_data_generator_fn = functools.partial(
      TocopoDataGenerator,
      stage_2_dir=stage_2_dir,
      split=constants.TRAIN_SPLIT_NAME,
      batch_size=batch_size,
      repeat_count=-1,
      drop_remainder=drop_remainder,
      shuffle_batch_count=100)

  validation_data_generator_fn = functools.partial(
      TocopoDataGenerator,
      stage_2_dir=stage_2_dir,
      split=constants.VALIDATION_SPLIT_NAME,
      batch_size=batch_size,
      repeat_count=1,
      drop_remainder=drop_remainder)

  test_data_generator_fn = functools.partial(
      TocopoDataGenerator,
      stage_2_dir=stage_2_dir,
      split=constants.TEST_SPLIT_NAME,
      batch_size=batch_size,
      repeat_count=1,
      drop_remainder=drop_remainder)

  output_token_encoder = output_encoder.Encoder(
      os.path.join(stage_2_dir, constants.VOCAB_FILES_DIRNAME,
                   constants.OUTPUT_TOKEN_VOCAB_FILENAME))

  return (train_data_generator_fn, validation_data_generator_fn,
          test_data_generator_fn, padding_spec, output_token_encoder)
